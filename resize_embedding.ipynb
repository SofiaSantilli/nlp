{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resize_embedding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoRPwwDgG2AK",
        "outputId": "8d3ec857-3aca-4837-ca2f-b0b934630eeb"
      },
      "source": [
        "!pip install transformers   #version: transformers-4.9.2\n",
        "print(\"\\n \\n \\n\")\n",
        "!pip install pytorch-lightning  #version: pytorch-lightning-1.4.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "\n",
            " \n",
            " \n",
            "\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.4.2-py3-none-any.whl (916 kB)\n",
            "\u001b[K     |████████████████████████████████| 916 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 24.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 19.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.62.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (5.4.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.6.0)\n",
            "Collecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.0-py3-none-any.whl (272 kB)\n",
            "\u001b[K     |████████████████████████████████| 272 kB 23.7 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.39.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.34.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.6.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.1)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 55.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 53.6 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=236d43401cfa173b61208f3c5839e01a4577d77d86ec97e615ca77879a540160\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, pyDeprecate, future, pytorch-lightning\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.7.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.1 pytorch-lightning-1.4.2 torchmetrics-0.5.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj_6FklUGlSf",
        "outputId": "10bb054d-a01d-4bbb-b9e1-41d8e34e8380"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as WN\n",
        "\n",
        "#to read the xml file\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "#to store the preprocessed data from the xml\n",
        "import csv\n",
        "\n",
        "from transformers import BertConfig, BertModel, BertPreTrainedModel, BertTokenizer\n",
        "import pytorch_lightning as p_light\n",
        "from typing import Callable, Optional, Union, List, Dict, Tuple"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA5klnFpG2uQ"
      },
      "source": [
        "# PARAMETERS\n",
        "bert_model_name = 'bert-base-uncased'\n",
        "# other possible bert models :  ('bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased',\n",
        "# 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-base-multilingual-cased')\n",
        "\n",
        "my_dropout = 0.1   # 0.1 === default dropout of the  'BertTokenizer.from_pretrained'\n",
        "hidden_size = 768  # 768 === default hidden_size of the  'BertTokenizer.from_pretrained'\n",
        "\n",
        "my_batch_size = 4  #8 sulla repo forse?????????"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB7vk0qyG9Rm"
      },
      "source": [
        "class Bert_for_WSD(BertPreTrainedModel):\n",
        "  def __init__(self, config):\n",
        "    super().__init__(config)\n",
        "    self.Bert = BertModel(config)\n",
        "    self.dropout = nn.Dropout(my_dropout)\n",
        "    self.add_output_layer = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    self.init_weights()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "_YLrcVXWHDuB",
        "outputId": "906fddd5-4a18-4da2-c1ac-68b6d9024128"
      },
      "source": [
        "bert_config = BertConfig.from_pretrained(bert_model_name, num_labels=2)\n",
        "bert_model = Bert_for_WSD.from_pretrained(bert_model_name, config=bert_config)\n",
        "#bert_model = get_bert_model(bert_model_name)\n",
        "\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
        "if '[TGT]' not in bert_tokenizer.additional_special_tokens:\n",
        "  bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[TGT]']})\n",
        "bert_model.resize_token_embeddings(len(bert_tokenizer))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing Bert_for_WSD: ['bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.2.output.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.output.dense.weight', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.pooler.dense.weight', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'cls.predictions.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'cls.seq_relationship.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.pooler.dense.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight']\n",
            "- This IS expected if you are initializing Bert_for_WSD from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Bert_for_WSD from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Bert_for_WSD were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.Bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.9.intermediate.dense.bias', 'bert.Bert.encoder.layer.8.attention.self.query.weight', 'bert.Bert.encoder.layer.3.attention.self.key.weight', 'bert.Bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.10.attention.self.value.bias', 'bert.Bert.encoder.layer.7.attention.self.value.bias', 'bert.Bert.encoder.layer.0.intermediate.dense.weight', 'bert.Bert.encoder.layer.4.attention.output.dense.weight', 'bert.Bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.2.attention.output.dense.bias', 'bert.Bert.encoder.layer.8.intermediate.dense.weight', 'bert.Bert.encoder.layer.8.attention.self.key.weight', 'bert.Bert.embeddings.word_embeddings.weight', 'bert.Bert.encoder.layer.7.attention.self.query.weight', 'bert.Bert.encoder.layer.2.intermediate.dense.weight', 'bert.Bert.encoder.layer.9.attention.self.value.weight', 'bert.Bert.encoder.layer.1.output.LayerNorm.bias', 'bert.Bert.encoder.layer.2.output.dense.weight', 'bert.Bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.1.intermediate.dense.bias', 'bert.Bert.encoder.layer.2.attention.self.key.weight', 'bert.Bert.encoder.layer.10.attention.self.key.bias', 'bert.Bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.10.attention.self.key.weight', 'bert.Bert.encoder.layer.11.attention.self.key.weight', 'bert.Bert.encoder.layer.1.attention.self.key.weight', 'bert.Bert.encoder.layer.2.output.dense.bias', 'bert.Bert.encoder.layer.1.attention.output.dense.weight', 'bert.Bert.encoder.layer.2.attention.self.value.bias', 'bert.Bert.encoder.layer.5.attention.self.key.bias', 'bert.Bert.encoder.layer.6.intermediate.dense.bias', 'bert.Bert.encoder.layer.3.output.LayerNorm.weight', 'bert.Bert.encoder.layer.5.attention.output.dense.weight', 'bert.Bert.encoder.layer.11.attention.self.value.bias', 'bert.Bert.encoder.layer.0.intermediate.dense.bias', 'bert.Bert.encoder.layer.4.attention.self.query.bias', 'bert.Bert.encoder.layer.4.attention.self.key.weight', 'bert.Bert.encoder.layer.11.attention.self.query.weight', 'bert.Bert.encoder.layer.3.attention.self.value.bias', 'bert.Bert.encoder.layer.0.output.LayerNorm.bias', 'bert.Bert.encoder.layer.11.output.LayerNorm.weight', 'bert.add_output_layer.weight', 'bert.Bert.encoder.layer.8.output.LayerNorm.weight', 'bert.Bert.encoder.layer.9.attention.self.key.bias', 'bert.Bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.Bert.pooler.dense.weight', 'bert.Bert.encoder.layer.7.output.LayerNorm.bias', 'bert.Bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.8.intermediate.dense.bias', 'bert.Bert.embeddings.LayerNorm.bias', 'bert.Bert.encoder.layer.5.attention.self.value.bias', 'bert.Bert.encoder.layer.0.attention.self.value.bias', 'bert.Bert.encoder.layer.3.attention.output.dense.bias', 'bert.Bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.10.output.LayerNorm.weight', 'bert.Bert.encoder.layer.10.output.dense.bias', 'bert.Bert.encoder.layer.5.output.LayerNorm.bias', 'bert.Bert.encoder.layer.9.output.LayerNorm.bias', 'bert.Bert.encoder.layer.3.attention.self.value.weight', 'bert.Bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.5.attention.self.value.weight', 'bert.Bert.encoder.layer.8.attention.output.dense.weight', 'bert.Bert.encoder.layer.6.output.LayerNorm.weight', 'bert.Bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.4.attention.self.value.weight', 'bert.Bert.encoder.layer.4.output.LayerNorm.weight', 'bert.Bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.5.attention.output.dense.bias', 'bert.Bert.encoder.layer.7.attention.output.dense.weight', 'bert.Bert.encoder.layer.1.intermediate.dense.weight', 'bert.Bert.encoder.layer.1.output.dense.weight', 'bert.Bert.encoder.layer.4.intermediate.dense.bias', 'bert.Bert.encoder.layer.5.intermediate.dense.bias', 'bert.Bert.encoder.layer.7.output.dense.weight', 'bert.Bert.encoder.layer.9.intermediate.dense.weight', 'bert.Bert.encoder.layer.11.attention.output.dense.bias', 'bert.Bert.encoder.layer.11.attention.self.key.bias', 'bert.Bert.encoder.layer.0.attention.output.dense.bias', 'bert.Bert.encoder.layer.5.output.LayerNorm.weight', 'bert.Bert.encoder.layer.8.output.dense.bias', 'bert.Bert.encoder.layer.11.output.LayerNorm.bias', 'bert.Bert.encoder.layer.1.attention.self.value.weight', 'bert.Bert.encoder.layer.4.attention.output.dense.bias', 'bert.Bert.encoder.layer.8.output.dense.weight', 'bert.Bert.embeddings.token_type_embeddings.weight', 'bert.Bert.encoder.layer.7.attention.self.value.weight', 'bert.Bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.7.attention.self.query.bias', 'bert.Bert.encoder.layer.5.output.dense.weight', 'bert.Bert.encoder.layer.4.output.dense.weight', 'bert.Bert.encoder.layer.7.attention.self.key.bias', 'bert.Bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.Bert.embeddings.LayerNorm.weight', 'bert.Bert.encoder.layer.11.attention.output.dense.weight', 'bert.Bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.11.output.dense.weight', 'bert.Bert.encoder.layer.9.output.dense.weight', 'bert.Bert.encoder.layer.1.output.LayerNorm.weight', 'bert.Bert.encoder.layer.2.attention.self.key.bias', 'bert.Bert.encoder.layer.3.intermediate.dense.bias', 'bert.Bert.encoder.layer.0.attention.self.key.weight', 'bert.Bert.encoder.layer.7.output.LayerNorm.weight', 'bert.Bert.encoder.layer.10.attention.output.dense.weight', 'bert.Bert.encoder.layer.10.attention.self.value.weight', 'bert.Bert.encoder.layer.7.intermediate.dense.weight', 'bert.Bert.encoder.layer.4.output.LayerNorm.bias', 'bert.Bert.encoder.layer.5.output.dense.bias', 'bert.Bert.encoder.layer.2.attention.self.value.weight', 'bert.Bert.encoder.layer.6.attention.output.dense.bias', 'bert.Bert.encoder.layer.4.attention.self.key.bias', 'bert.Bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.4.intermediate.dense.weight', 'bert.Bert.encoder.layer.11.intermediate.dense.bias', 'bert.Bert.encoder.layer.11.output.dense.bias', 'bert.Bert.encoder.layer.1.attention.self.value.bias', 'bert.Bert.encoder.layer.0.output.LayerNorm.weight', 'bert.Bert.encoder.layer.9.attention.output.dense.bias', 'bert.Bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.8.attention.self.value.bias', 'bert.Bert.encoder.layer.7.intermediate.dense.bias', 'bert.Bert.encoder.layer.2.intermediate.dense.bias', 'bert.Bert.encoder.layer.3.attention.self.query.bias', 'bert.Bert.encoder.layer.2.attention.output.dense.weight', 'bert.Bert.encoder.layer.5.intermediate.dense.weight', 'bert.Bert.pooler.dense.bias', 'bert.Bert.encoder.layer.8.attention.self.query.bias', 'bert.Bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.10.intermediate.dense.bias', 'bert.Bert.encoder.layer.0.output.dense.bias', 'bert.Bert.encoder.layer.9.attention.self.query.bias', 'bert.Bert.encoder.layer.11.attention.self.value.weight', 'bert.Bert.encoder.layer.11.intermediate.dense.weight', 'bert.Bert.encoder.layer.9.output.LayerNorm.weight', 'bert.Bert.encoder.layer.11.attention.self.query.bias', 'bert.Bert.encoder.layer.3.intermediate.dense.weight', 'bert.Bert.encoder.layer.1.attention.self.key.bias', 'bert.Bert.encoder.layer.9.output.dense.bias', 'bert.Bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.4.attention.self.value.bias', 'bert.Bert.encoder.layer.3.output.dense.weight', 'bert.Bert.encoder.layer.10.attention.self.query.weight', 'bert.Bert.encoder.layer.6.attention.self.value.bias', 'bert.Bert.encoder.layer.9.attention.output.dense.weight', 'bert.Bert.encoder.layer.1.attention.output.dense.bias', 'bert.Bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.10.attention.self.query.bias', 'bert.Bert.encoder.layer.5.attention.self.query.weight', 'bert.Bert.encoder.layer.6.attention.self.query.bias', 'bert.Bert.encoder.layer.2.attention.self.query.weight', 'bert.Bert.encoder.layer.6.attention.self.query.weight', 'bert.Bert.encoder.layer.0.attention.output.dense.weight', 'bert.Bert.encoder.layer.3.attention.self.key.bias', 'bert.Bert.encoder.layer.10.intermediate.dense.weight', 'bert.Bert.encoder.layer.8.attention.output.dense.bias', 'bert.Bert.encoder.layer.6.attention.self.key.weight', 'bert.Bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.2.attention.self.query.bias', 'bert.Bert.encoder.layer.5.attention.self.key.weight', 'bert.Bert.encoder.layer.3.attention.self.query.weight', 'bert.Bert.encoder.layer.9.attention.self.value.bias', 'bert.Bert.encoder.layer.10.output.dense.weight', 'bert.Bert.encoder.layer.0.attention.self.query.weight', 'bert.Bert.encoder.layer.0.attention.self.query.bias', 'bert.Bert.encoder.layer.1.output.dense.bias', 'bert.Bert.encoder.layer.6.attention.self.value.weight', 'bert.Bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.5.attention.self.query.bias', 'bert.Bert.encoder.layer.6.output.LayerNorm.bias', 'bert.Bert.encoder.layer.0.output.dense.weight', 'bert.Bert.encoder.layer.7.output.dense.bias', 'bert.Bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.Bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.Bert.encoder.layer.6.output.dense.weight', 'bert.Bert.encoder.layer.0.attention.self.value.weight', 'bert.Bert.encoder.layer.2.output.LayerNorm.bias', 'bert.Bert.encoder.layer.8.attention.self.value.weight', 'bert.Bert.encoder.layer.1.attention.self.query.weight', 'bert.Bert.encoder.layer.9.attention.self.key.weight', 'bert.Bert.encoder.layer.6.intermediate.dense.weight', 'bert.Bert.encoder.layer.0.attention.self.key.bias', 'bert.Bert.encoder.layer.10.output.LayerNorm.bias', 'bert.Bert.encoder.layer.3.output.LayerNorm.bias', 'bert.Bert.encoder.layer.1.attention.self.query.bias', 'bert.Bert.embeddings.position_embeddings.weight', 'bert.Bert.encoder.layer.2.output.LayerNorm.weight', 'bert.Bert.encoder.layer.3.output.dense.bias', 'bert.Bert.encoder.layer.6.attention.self.key.bias', 'bert.Bert.encoder.layer.7.attention.self.key.weight', 'bert.Bert.encoder.layer.6.output.dense.bias', 'bert.add_output_layer.bias', 'bert.Bert.encoder.layer.8.output.LayerNorm.bias', 'bert.Bert.encoder.layer.9.attention.self.query.weight', 'bert.Bert.encoder.layer.10.attention.output.dense.bias', 'bert.Bert.encoder.layer.3.attention.output.dense.weight', 'bert.Bert.encoder.layer.4.output.dense.bias', 'bert.Bert.encoder.layer.7.attention.output.dense.bias', 'bert.Bert.encoder.layer.6.attention.output.dense.weight', 'bert.Bert.encoder.layer.8.attention.self.key.bias', 'bert.Bert.encoder.layer.4.attention.self.query.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5842335eeec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'[TGT]'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'additional_special_tokens'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'[TGT]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mresize_token_embeddings\u001b[0;34m(self, new_num_tokens)\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPointer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0mEmbeddings\u001b[0m \u001b[0mModule\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \"\"\"\n\u001b[0;32m--> 710\u001b[0;31m         \u001b[0mmodel_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_num_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_resize_token_embeddings\u001b[0;34m(self, new_num_tokens)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0mold_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m         \u001b[0mnew_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_resized_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_num_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mget_input_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}